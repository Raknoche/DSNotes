{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "The goal of linear regression is to develop a linear model f(x) that describes the data based on a number of features.  A general model can be written as\n",
    "\n",
    "$f(x) = \\theta_0 x_0 +  \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + ...$\n",
    "\n",
    "Where\n",
    "\n",
    "$x_0 = 1$ is used for convenience ($\\theta_0$ is essentially the intercept term)\n",
    "\n",
    "$n$ = the number of training examples\n",
    "\n",
    "$m$ = number of features/variables\n",
    "\n",
    "$X_i$ = input of the ith sample\n",
    "\n",
    "$y_i$ is the output of the ith sample\n",
    "\n",
    "$x^j$ = the jth feature\n",
    "\n",
    "$x_i^j$ = value of feature j, for the ith sample\n",
    "\n",
    "\n",
    "For convience, we define $x^0=1$ to fit the pattern shown above. This allows us to write the parameters $\\theta^j$ as a vector $\\mathbf{\\Theta}$, and the features as a vector $\\mathbf{X}$.  Then, using linear algebra:\n",
    "\n",
    "$f(x) = \\mathbf{\\Theta}^T \\mathbf{X}$\n",
    "\n",
    "Note that the features themselves do not need to be linear.  For instance, the features could be $x^0 = 1$, $x^1 = \\text{yards}$, and $x^2 = \\text{yards}^2$.  It is called linear regression because we assume the predicted variable is linearly dependent on the features.\n",
    "\n",
    "# Defining a Cost function\n",
    "\n",
    "We need to choose the fit parameters of $\\mathbf{\\Theta_j}$so that f(x) is as close to the true values of $y$ as possible.  We define what \"close\" means by choosing a **cost function** that we wish to minimize.  One popular cost function is the **mean squared error**, which is defined by the squared difference between f(x) and the training data y:\n",
    "\n",
    "$ J(\\mathbf{\\Theta}) = \\frac{1}{2n}\\sum_{i=1}^n \\left(f(x_i) - y_i \\right)^2$\n",
    "\n",
    "\n",
    "When this is used as the cost function, the linear regression is known as a **least squares regression**.\n",
    "\n",
    "\n",
    "# Minimizing the Cost function\n",
    "\n",
    "A common algorithm to minimize any cost function is **gradient descent**.  The algorithm changes the parameters $\\mathbf{\\Theta}$ based on the derivate of the cost function.  In this way, it takes large steps when it is far from a minimum, and small steps when it is close to a minimum.  Mathematically, $\\Theta$ is updated according to\n",
    "\n",
    "$ \\theta^j := \\theta^j - \\alpha \\frac{\\delta}{\\delta \\theta^j}J(\\mathbf{\\Theta}) = \\theta^j - \\alpha \\frac{1}{n}\\sum_{i=1}^n \\left(f(x_i) - y_i \\right) x^j_i$\n",
    "\n",
    "Note that the algorithm simulatneously calculates the new $\\theta^j$ values before updating the cost function.  Do NOT update $J(\\mathbf{\\Theta})$ before all of the new $\\theta^j$ values have been determined.  It's also worth noting that gradient descent could go towards a local minimum if one exists in the cost function.  However, the cost function for linear regression is always bowl shaped, so there are no local minima to worry about.\n",
    "\n",
    "It is often help to transform the model features so that the mean of each feature is subtracted, and the ranges of all features are on the same scale:\n",
    "\n",
    "$x^j := \\frac{(x^j - \\mu^j)}{\\sigma(x^j)}$\n",
    "\n",
    "This ensures the cost function is a symetric bowl, rather than an elongated ellipse.  In turn, the minimization algorithm will perform better.\n",
    "\n",
    "A final note about minimization: If you plot the cost function versus the number of minimzation iterations, it should approach zero steadily.  If the cost function does not steadily decrease with each iteration, then the minimization algorithm is overshooting the minimum and the learn rate $\\alpha$ needs to be lowered.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Maximum Likelihood Estimation\n",
    "\n",
    "A more generalized approach to regression fitting is called **Maximum Likelihood Estimation (MLE)**.  In this approach, a **probability distribution function (PDF)** is used to describe the probability of observing an outcome $y_i$ given the features $X$ and the fit parameters $\\Theta$.  Using the PDF, we calculate the probability of each observation in our sample occuring.  Then, the probability of our entire sample being observed is given by the product of the individual obesrvation probabilities.  The fit parameters which generate a PDF that maximizes the probability of our sample being observed are the best fit parameters.\n",
    "\n",
    "In practice, it is difficult to maximize the product of multiple PDFs.  Instead, we can take the log of the PDF.  Since $log(PDF1*PDF2*PDF3*...) = log(PDF1) + log(PDF2) + log(PDF3) + ...$ this allows us to work with a summation of multiple PDFs, rather than one large product.  For performance reasons, we also tend to minimize the negative log likelihood, rather than maximize the log likelihood.\n",
    "\n",
    "\n",
    "In the case of a linear regression, we assume that the the errors of the sample are normally distributed with variance $\\sigma^2$, independent of the values of our features, and independent across our observations.  Since we assume that the errors of the sample follow a normal distribution, the total probability of observing our sample is given by the product of normal distributions:\n",
    "\n",
    "$\\prod_{i=1}^{n} p(y_i \\vert X_i; \\Theta, \\sigma^2)$ = $\\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(f(X_i)-y_i)^2}{2\\sigma^2}}$\n",
    "\n",
    "Taking the negative log of the equation above, we get\n",
    "\n",
    "$ -L(\\Theta,\\sigma^2) =  \\frac{n}{2}\\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (f(X_i)-y_i)^2 $\n",
    "\n",
    "Which is very similar to the cost function we defined using least squares regression, which justifies the use of that cost function -- particularly in the case of normally distributed errors.\n",
    "\n",
    "\n",
    "# Analysis of Variance\n",
    "\n",
    "Assuming the errors are normally distributed, we can perform an analysis of variance on the regression.  The first step is creating the Analysis of Variance (ANOVA) Table. This has three parts, the total variance, the variance explained by our regression, and the residual error variance.\n",
    "\n",
    "1) Total Sum of Squares: $\\text{SST} = \\sum_{i=1}^n (y_i - \\overline{y})^2$\n",
    "\n",
    "2) Regression Sum Of Squares: $\\text{SSR} = \\sum_{i=1}^n (f(X_i)-\\overline{y})^2$\n",
    "\n",
    "3) Error Sum of Squares: $\\text{SSE} = \\sum_{i=1}^n (f(x_i) - y_i)^2$\n",
    "\n",
    "4) Note that SST = SSR + SSE\n",
    "\n",
    "Where $\\overline{y}$ is the average of the observations.  The ANOVA table can be used to calculate a number of important test statistics. With $p = m - 1$ representing the number of features (excluding the constant term $x_0$):\n",
    "\n",
    "1) Mean Squared Error: $\\text{MSE} = \\frac{SSE}{n-p-1}$\n",
    "\n",
    "2) Regression Mean Square: $\\text{MSR} = \\frac{SSR}{p}$\n",
    "\n",
    "3) Total Mean Square: $\\text{MST} = \\frac{SST}{n-1}$\n",
    "\n",
    "The **Coefficient of determination**, commonly called **$\\text{R}^2$**, tells us how much of the variation in the data can be explained by our model.  It is given by:\n",
    "\n",
    "$\\text{R}^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}} = \\frac{\\text{SSR}}{\\text{SST}}$\n",
    "\n",
    "To test if our regression is signficant, we compare it to the null hypothesis that there is no relation between to predicted variable $y$ and the features $X$.  This involved calculating an F-Score given by:\n",
    "\n",
    "$\\text{F} = \\frac{\\text{MSR}}{\\text{MSE}} = \\frac{\\text{R}^2(n-p-1)}{(1-\\text{R}^2)p}$\n",
    "\n",
    "The F-score is converted to a p-value using a look up table.  The p-value tells us the likelihood of the null hypothesis.  Low p-values ($<$0.05) indicate that the fit parameter is important and should indeed be included in the fit.\n",
    "\n",
    "\n",
    "Source: https://www3.nd.edu/~rwilliam/stats2/l02.pdf\n",
    "\n",
    "\n",
    "# Regularization\n",
    "\n",
    "When performing regularization, we favor simpler models by making the theta parameters smaller.  In linear regression, regularization is performed by adding a regularization function, $R$, that penalizes complexity in the model:\n",
    "\n",
    "$J(\\Theta) = \\frac{1}{2n} \\sum_{i=1}^{n}(f(x_i)-y_i)^2 + \\lambda *R$\n",
    "\n",
    "For linear regression, two common regularization methods include:\n",
    "\n",
    "L1 regularization: $R = \\sum_{j=1}^{m} \\left| \\theta_j \\right|$\n",
    "\n",
    "L2 regularization: $R = \\sum_{j=1}^{m}\\theta_j^2$\n",
    "\n",
    "Note that L1 and L2 regularization can be used outside of linear regression model, but they are conceptual easy to understand within this context.  When L1 regularization is used in a least squares linear regression, the model is known as a **LASSO regression**.  When L2 regularization is used in a least squares linear regression, the model is known as a **ridge regression**.\n",
    "\n",
    "\n",
    "In the case of L2 regularization (ridge regression), the parameters update according to:\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha \\frac{\\delta}{\\delta \\theta_j}J(\\Theta)$\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha \\left[ \\frac{1}{n}\\sum_{i=1}^{n}(f(x_i)-y_i)x_i^j + \\lambda \\theta_j \\right]$\n",
    "\n",
    "This is equivalent to \n",
    "\n",
    "$\\theta_j := \\theta_j(1-\\alpha\\frac{\\lambda}{n}) - \\frac{\\alpha}{n}\\sum_{i=1}^{n}(f(x_i)-y_i)x_i^j$\n",
    "\n",
    "Since $(1-\\alpha\\frac{\\lambda}{n})$ is always less than one, the parameters $\\theta_j$ are being scaled down by a multiplicative factor.  This means that during each update, the parameters shrink, but they are never set to zero.\n",
    "\n",
    "\n",
    "In the case of L1 regularization (LASSO regression), the paramters update according to:\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha \\frac{\\delta}{\\delta \\theta_j}J(\\Theta)$\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha \\left[ \\frac{1}{n}\\sum_{i=1}^{n}(f(x_i)-y_i)x_i^j + \\lambda \\frac{\\left| \\theta_j \\right|}{ \\theta_j} \\right]$\n",
    "\n",
    "This is equivalent to \n",
    "\n",
    "$\\theta_j := \\theta_j-\\frac{\\alpha\\lambda\\theta_j}{n \\left| \\theta_j \\right|}\\ - \\frac{\\alpha}{n}\\sum_{i=1}^{n}(f(x_i)-y_i)x_i^j$\n",
    "\n",
    "Since $\\frac{\\theta_j}{\\left| \\theta_j \\right|} = \\pm 1$, L1 regularization translates the coefficients towards zero by a constant value.  Unlike the multiplicative factor of L2 regularization, this translation allows parameters to be set to exactly zero, which helps simplify the number of features in a model.\n",
    "\n",
    "\n",
    "The below image illustrates the difference between L1 and L2 normalization.  L1 normalization encourages sparsity of features (setting some parameters close to zero), while L2 normalization only encourages the shrinking of parameters.\n",
    "<img src=\"regularization.jpg\">\n",
    "\n",
    "\n",
    "There are two downsides of L1 regularization.  First, it is non-differentiable, which means some minimization algorithms such as gradient descent will not work.  It also doesn't have an analytic solution, so the minimization will take longer than L2 regularization.  Second, L1 normalization can introduce local minima to the cost function.  A solution to this is to combine L1 and L2 regularization terms (with their own $\\lambda_1$ and $\\lambda_2$ parameters), which is known as **elastic net regularization**.\n",
    "\n",
    "\n",
    "In practice, L2 regularization tends to perform better, unless there are a large number of features that we need to automatically select a subset of features from. In the latter case, elastic net regularization is preferable to L1 regularization, since it forces the cost function to remain covex with only one global minimium. \n",
    "\n",
    "\n",
    "One last note -- if the regularization parameter $\\lambda$ is too large, we can end up underfitting the model.  This is a hyperparameter than needs to be tuned to the CV set.\n",
    "\n",
    "\n",
    "\n",
    "# Evaluating a regression model\n",
    "\n",
    "When evaluating a model\n",
    "\n",
    "1) Split the data set it training (~60%), cross validation (~20%), and test (~20%) sets\n",
    "\n",
    "2) Set the test set aside, and do not work with it until the model is developed\n",
    "\n",
    "3) Use the training set to determine the best fit parameters.  \n",
    "\n",
    "4) Tune hyper parameters, such as the learning rate, to minimize the CV set error.  \n",
    "\n",
    "5) Once the hyperparameters are tuned to the CV set, evaluate the model's performance on the test set to see how the final model performs on a random sample.\n",
    "\n",
    "\n",
    "If our model is not performing well on our CV set, it might be underfitting or overfitting the training data.  In the case of underfitting, the model is not complex enough to explain the data.  In the case of overfitting, the model may be too complex, allowing it to fit the training points exactly, but fail to generalize to new data such as the CV set.\n",
    "\n",
    "To determine if a hyperparameter is causing overfitting or underfitting, you can plot the training set error and CV set error versus the hyperparameter.  If both the training and CV set errors are high, the model is not able to explain either of the data sets well, so it is underfitting.  If the training set error is low, but the test set error is high, then the model can explain the training set well but fails to generalize to new data -- this means the model is overfitting.  Ideally, we want both the test set and the training set errors to be low.\n",
    "\n",
    "Another useful plot is known as the learning curve.  Suppose we plot the Training and CV errors versus the size of the training set.  For very low size of the training set, the model will be able to fit all of the training points easily, leading to a low training error.  However, the model didn't recieve enough data to generalize well, leading to a high CV error.  As the training set size grows, the Training Error will asympotote to a larger number, and the CV Error will asympotote to a smaller number.\n",
    "\n",
    "For an underfit model, the model will never perform well on either the training or the CV sets.  This means that as the size of the training set is increased, both errors will converge to the same high error rate. If you find that your model is underfitting, try to add additional or more complicated features, or try to decrease the regularization parameter.\n",
    "\n",
    "For an overfit model, the model will always perform well on the training set, and always perform poorly on the CV set.  In the case, while both curves will converge, a gap will remain between the two. Note that this means increasing the amount of training data we have can help, since it is harder to overfit when there is a large amount of data. If you find that your model is overfitting the training set, try smaller set of features, higher number of training examples, or increasing the regularization parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn' has no attribute 'linear_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f103a9b89559>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#Fit the model (note: Lasso and ridge_regression are also avaiable)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sklearn' has no attribute 'linear_model'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "#Generating features and observations\n",
    "x = y = np.linspace(0,10,3)\n",
    "X,Y = np.meshgrid(x,y)\n",
    "Z = 5.67*X +0.43*Y + np.random.normal(0,size=X.shape)\n",
    "\n",
    "#Make the (n_samples x n_features) array \"X\"\n",
    "feat_array = np.stack((X.flatten(),Y.flatten()),axis=1)\n",
    "\n",
    "#Fit the model (note: Lasso and ridge_regression are also avaiable)\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(feat_array,Z.flatten())\n",
    "\n",
    "#Print the coefficients\n",
    "print(\"Fit Parameters: \",model.coef_)\n",
    "\n",
    "#Predict at a new value\n",
    "model.predict(np.array([10,10]).reshape(1,-1))\n",
    "\n",
    "#Get R^2:\n",
    "model.score(feat_array,Z.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation with SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 10.071635431756704\n",
       " hess_inv: array([[ 0.00025683, -0.00128415],\n",
       "       [-0.00128415,  0.00867073]])\n",
       "      jac: array([ -3.33786011e-06,  -1.19209290e-07])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 28\n",
       "      nit: 5\n",
       "     njev: 7\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([  5.48374866,  14.03415076])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assuming normally distributed errors\n",
    "import scipy.optimize\n",
    "x = np.linspace(0,10,40)\n",
    "y = 5.5*x + 14 + np.random.normal(0,0.3,size=x.shape)\n",
    "\n",
    "\n",
    "def NLL(theta,x,y):\n",
    "    model = theta[0]*x+theta[1]\n",
    "    return (1/(2*0.3**2))*np.sum( (y-model)**2)\n",
    "\n",
    "scipy.optimize.minimize(NLL,[5,12], args=(x,y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
