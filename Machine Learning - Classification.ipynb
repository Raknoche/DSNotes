{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with unbalanced classes\n",
    "\n",
    "If the classes that we are predicting are very unbalanced, it can complicate the machine learning process.  For instance, if the algorithm predicts the majority class all of the time it can be fairly accurate, but completely useless.  If this is the case, there are three metrics that are more useful for evaluating the classifier's performance:\n",
    "\n",
    "The **precision** of our classifier is the fraction of samples it correctly identified as positive out of all of the samples that it identified as positive:\n",
    "\n",
    "$ \\text{P} = \\frac{\\text{number of true positives}}{\\text{number of true positive + number of false positives}} $\n",
    "\n",
    "The **recall** of our classifier is the fraction of samples that were correctly identified as positive out of the total number of positive samples.\n",
    "\n",
    "$ \\text{R} = \\frac{ \\text{number of true positives}}{\\text{number of true positive + number of false negatives} }$\n",
    "\n",
    "When tuning a classifier, we often trade off precision for recall, or vice versa.  Which metric you prefer will depend on the model you are developing, but if both high precision and high recall are desired, a popular way to combine them is with the **F1 score**:\n",
    "\n",
    "$\\text{F1} = 2 \\frac{P*R}{P+R}$\n",
    "\n",
    "\n",
    "The classes imbalance also plays a role when training a model. Depending on the machine learning algorithm, the model may want to predict the majority class all of the time.  For instance, in a Naive Bayes Classifier, if the probability of the minority class appearing is extremely small, the probability of a document belonging to that class, regardless of the features, will also be small.  To combat this, we typically try to balance the classes in our training set.  There are two methods to do so:\n",
    "\n",
    "1) Undersample the majority class.  This means every time we put in a minority sample into our training set, we put exactly one majority sample in the set as well.\n",
    "\n",
    "2) Oversample the minority class.  This means every time we put a minority sample into our training set, we use multiple copies of it.  This is useful when we have very little data and do not want to throw out any of the positive class.  However, it may give too much weight to features in the copied samples, which can be avoided with the undersampling method described above.\n",
    "\n",
    "It's worth trying undersampling and oversampling in your model, and testing the result on your CV set.  Note that the final CV and test sets should include an accurate ratio of positive and negative classes, so that they reflect how the classifier will perform on a random set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating a classifier: https://www.youtube.com/watch?v=OAl6eAyP-yo\n",
    "\n",
    "ROC plots show the false postive rate (x-axis) versus the true positive rate (y-axis) of a classifier.  The curve is drawn by scanning over a range of threshold, and is used to visualize the effect of changing the classifier's threshold.\n",
    "\n",
    "The area under the ROC curve is known as the **area under the curve** (AUC), and tells you how well the classifier separates classes.  A strong classifier will have a high AUC, reflecting an ROC that hugs the upper left corner.  A classifier that is no better than a coin flip with have a linear, diagonal ROC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
