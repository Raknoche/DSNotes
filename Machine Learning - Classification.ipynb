{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression is used for classificatin problems.  The model is given by a sigmoid function:\n",
    "\n",
    "$f(X_i) = \\frac{1}{1+e^{\\theta_0x_0+\\theta_1x_1+...}}$\n",
    "\n",
    "The prediction of logistic regression is interpreted as the probability of the sample belonging to the class $y_i$ given the features $X_i$:\n",
    "\n",
    "$ \\text{P}(y_i | X_i) = f(X_i)$\n",
    "\n",
    "We must choose a threshold at which to consider sample $X_i$ as belonging to class $y_i$.  Typically we set this threshold at a 50% probability, but this can change based on the circumstances.\n",
    "\n",
    "To ensure that the cost function is a convex bowl, the cost function for logistic regression is given by:\n",
    "\n",
    "$J(\\Theta) = \\frac{1}{n}\\sum_{i=1}^n \\left\\{\\begin{aligned}\n",
    "&-\\log(f(x_i)) &&: y_i = 1\\\\\n",
    "&-\\log(1-f(x_i)) &&: y_i = 0\n",
    "\\end{aligned}\n",
    "\\right.$\n",
    "\n",
    "This is equivalent to:\n",
    "\n",
    "$J(\\Theta) = \\frac{1}{n}\\sum_{i=1}^n \\left[ \n",
    "-y_i\\log\\left(f(x_i)\\right)-(1-y_i)\\log\\left(1-f(x_i)\\right) \\right]$\n",
    "\n",
    "With this cost function, if we predict y=1 when y=1, the cost is zero.  If we predict y=0, when y=1, the cost blows up to infinity (and vice verse for y=0 samples).  Note that this cost function can also be derived with maximum likelihood estimation.\n",
    "\n",
    "We use gradient descent to minimize this, which ends up having the same update rule as linear regression (with a different model for $f(x_i)$):\n",
    "\n",
    "$\\theta_j = \\theta_j - \\alpha\\sum_{i=1}^n(f(x_i)-y_i)x_i^j$\n",
    "\n",
    "Note that it's possible to train a logistic regression model to classify more than one class.  In this case, we take a \"one versus all\" approach.  Train a classifier for each class that predicts the probability of the sample being in that class or not being in that class.  Then, for predicting new samples, run each classifier and choose the class that has the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate some data\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.random.normal(10,size=30)\n",
    "y = np.random.normal(10,size=30)\n",
    "label = np.ones(30)\n",
    "\n",
    "x2 = np.random.normal(0,size=30)\n",
    "y2 = np.random.normal(0,size=30)\n",
    "label2 = np.zeros(30)\n",
    "\n",
    "x=np.concatenate([x,x2])\n",
    "y=np.concatenate([y,y2])\n",
    "labels=np.concatenate([label,label2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x118652d68>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEACAYAAACnJV25AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDC+0gy5KAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117f5a8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "\n",
    "model = sklearn.linear_model.LogisticRegression()\n",
    "model.fit(np.stack([x,y],axis=1),labels)\n",
    "\n",
    "xx = yy= np.linspace(-4,14,200)\n",
    "X,Y = np.meshgrid(xx,yy)\n",
    "Z = model.predict(np.stack([X.flatten(),Y.flatten()],axis=1))\n",
    "Z = Z.reshape(X.shape)\n",
    "\n",
    "plt.pcolormesh(X,Y,Z,cmap=plt.cm.OrRd)\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with unbalanced classes\n",
    "\n",
    "If the classes that we are predicting are very unbalanced, it can complicate the machine learning process.  For instance, if the algorithm predicts the majority class all of the time it can be fairly accurate, but completely useless.  If this is the case, there are three metrics that are more useful for evaluating the classifier's performance:\n",
    "\n",
    "The **precision** of our classifier is the fraction of samples it correctly identified as positive out of all of the samples that it identified as positive:\n",
    "\n",
    "$ \\text{P} = \\frac{\\text{number of true positives}}{\\text{number of true positive + number of false positives}} $\n",
    "\n",
    "The **recall** of our classifier is the fraction of samples that were correctly identified as positive out of the total number of positive samples.\n",
    "\n",
    "$ \\text{R} = \\frac{ \\text{number of true positives}}{\\text{number of true positive + number of false negatives} }$\n",
    "\n",
    "When tuning a classifier, we often trade off precision for recall, or vice versa.  Which metric you prefer will depend on the model you are developing, but if both high precision and high recall are desired, a popular way to combine them is with the **F1 score**:\n",
    "\n",
    "$\\text{F1} = 2 \\frac{P*R}{P+R}$\n",
    "\n",
    "\n",
    "The classes imbalance also plays a role when training a model. Depending on the machine learning algorithm, the model may want to predict the majority class all of the time.  For instance, in a Naive Bayes Classifier, if the probability of the minority class appearing is extremely small, the probability of a document belonging to that class, regardless of the features, will also be small.  To combat this, we typically try to balance the classes in our training set.  There are two methods to do so:\n",
    "\n",
    "1) Undersample the majority class.  This means every time we put in a minority sample into our training set, we put exactly one majority sample in the set as well.\n",
    "\n",
    "2) Oversample the minority class.  This means every time we put a minority sample into our training set, we use multiple copies of it.  This is useful when we have very little data and do not want to throw out any of the positive class.  However, it may give too much weight to features in the copied samples, which can be avoided with the undersampling method described above.\n",
    "\n",
    "It's worth trying undersampling and oversampling in your model, and testing the result on your CV set.  Note that the final CV and test sets should include an accurate ratio of positive and negative classes, so that they reflect how the classifier will perform on a random set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a classifier: https://www.youtube.com/watch?v=OAl6eAyP-yo\n",
    "\n",
    "ROC plots show the false postive rate (x-axis) versus the true positive rate (y-axis) of a classifier.  The curve is drawn by scanning over a range of threshold, and is used to visualize the effect of changing the classifier's threshold.\n",
    "\n",
    "The area under the ROC curve is known as the **area under the curve** (AUC), and tells you how well the classifier separates classes.  A strong classifier will have a high AUC, reflecting an ROC that hugs the upper left corner.  A classifier that is no better than a coin flip with have a linear, diagonal ROC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
